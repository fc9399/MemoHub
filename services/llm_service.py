# LLM service
import requests
import json
from typing import Dict, Any, List
from openai import OpenAI
from config import settings

class LLMService:
    """
    LLM service - Uses NVIDIA NIM for text generation
    """
    
    def __init__(self):
        self.api_key = settings.NVIDIA_API_KEY
        
        # Use same configuration as embedding service
        if settings.ENVIRONMENT == "production":
            self.base_url = settings.NIM_EMBEDDING_URL
            print(f"ðŸš€ Using production NIM: {self.base_url}")
        else:
            self.base_url = settings.NVIDIA_API_BASE_URL
            print(f"ðŸ”§ Using development API: {self.base_url}")
        
        self.client = OpenAI(
            base_url=self.base_url,
            api_key=self.api_key
        )
        self.model = "nvidia/llama-3.1-nemotron-nano-8b-v1"  # âœ… Competition requirement
        
    def generate_response(
        self,
        user_input: str,
        context: str = "",
        conversation_history: List[Dict[str, str]] = None
    ) -> str:
        """
        Generate AI response
        
        Args:
            user_input: User input
            context: Context information
            conversation_history: Conversation history
            
        Returns:
            str: AI response
        """
        try:
            # Build system prompt
            system_prompt = self._build_system_prompt(context)
            
            # Build messages
            messages = [{"role": "system", "content": system_prompt}]
            
            # Add conversation history
            if conversation_history:
                for turn in conversation_history[-6:]:  # Last 6 conversation turns
                    messages.append({"role": "user", "content": turn.get("user_input", "")})
                    messages.append({"role": "assistant", "content": turn.get("response", "")})
            
            # Add current user input
            messages.append({"role": "user", "content": user_input})
            
            # Call NVIDIA NIM API
            response = self._call_nvidia_api(messages)
            
            return response
            
        except Exception as e:
            print(f"âŒ LLM generation failed: {e}")
            return "Sorry, I encountered some technical issues. Please try again later."
    
    def _build_system_prompt(self, context: str) -> str:
        """Build system prompt"""
        base_prompt = """You are an intelligent personal memory assistant named UniMem AI. Your main functions are:

        ##  Core Task
        Directly use the provided memory information to answer user questions - don't ask the user back!

        ## âœ… Correct Answering Approach
        When the user asks: "What is Gregory's course about?"
        If there is information in memory, answer directly:
        "Gregory's course is about time series analysis, panel data, and forecasting methods! Specifically, it includes Class #3 and Class #4, covering statistical analysis and forecasting techniques for time series data."

        ## âŒ Wrong Answering Approach
        Don't say:
        - "According to the provided information..." (too stiff)
        - "Please tell me more about..." (don't ask back)
        - "I need to confirm..." (don't question the memory)
        - "Gregory's course is probably about..." (don't be vague)

        ## ðŸ“‹ Answering Principles
        1. **Direct Answer**: If information is in memory, answer directly and clearly
        2. **Confident Expression**: Use affirmative tone, don't say "maybe", "probably"
        3. **Specific Details**: Use specific details from memory
        4. **Natural Conversation**: Communicate like a friend, use emojis appropriately ðŸ˜Š
        5. **Admit Not Knowing**: Only say you don't know when there's truly no information in memory

        ## ðŸ”‘ Key Instructions
        - Prioritize using the "Current Memory Information" provided below
        - Memory information is fact, don't question it
        - Don't make users repeat information that's already in memory

        """

        if context:
            # Has context - emphasize using this information
            base_prompt += f"""

    ## ðŸ“š Current Memory Information
    The following are relevant memories found by the system. Please use this information directly to answer the user's question:

    {context}

    **Important Note**: The above memory information is the factual basis you should use. Please answer the user directly and confidently using this information, don't ask the user about content that's already in memory!
    """
        else:
            # No context - politely inform
            base_prompt += """

    ## âš ï¸ Notice
    Currently no relevant memory information was found. Please politely tell the user you don't have relevant memories yet, and ask if they need information about something else.
    """
        
        return base_prompt
        

    
    def _call_nvidia_api(self, messages: List[Dict[str, str]]) -> str:
        """Call NVIDIA NIM API"""
        try:
            # Try using chat completions
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=1000,
                temperature=0.7
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            print(f"âŒ NVIDIA NIM chat completions failed: {e}")
            # If chat completions unavailable, use simplified text generation
            return self._generate_simple_response(messages)
    
    def _generate_simple_response(self, messages: List[Dict[str, str]]) -> str:
        """Generate simplified AI response (not dependent on LLM API)"""
        try:
            # Get last user message
            user_message = ""
            for message in reversed(messages):
                if message.get("role") == "user":
                    user_message = message.get("content", "")
                    break
            
            # Generate simple response based on user input
            user_input_lower = user_message.lower()
            
            # Greetings
            if any(word in user_input_lower for word in ["ä½ å¥½", "hello", "hi", "æ‚¨å¥½"]):
                return "Hello! I'm UniMem AI assistant, I can help you manage and retrieve personal memories. How can I help you?"
            
            # Questions about JavaScript
            elif any(word in user_input_lower for word in ["javascript", "js", "frontend", "programming", "å‰ç«¯", "ç¼–ç¨‹"]):
                return "Regarding JavaScript, I can help you search relevant technical documentation and memories. JavaScript is a widely used programming language, mainly for web development."
            
            # Questions about React
            elif any(word in user_input_lower for word in ["react", "framework", "component", "æ¡†æž¶", "ç»„ä»¶"]):
                return "React is a JavaScript library for building user interfaces. It uses component-based development patterns, improving code maintainability and reusability."
            
            # Search related
            elif any(word in user_input_lower for word in ["æœç´¢", "æŸ¥æ‰¾", "æ‰¾", "search", "find"]):
                return "I can help you search relevant memories and documents. Please tell me what you'd like to know, and I'll look for relevant information in your memories."
            
            # Default response
            else:
                return f"I understand your question: '{user_message}'. Although I currently cannot use advanced AI features, I can help you search relevant memories and documents. Please tell me what specific content you'd like to know about."
                
        except Exception as e:
            print(f"âŒ Simple response generation failed: {e}")
            return "Sorry, I encountered some technical issues. Please try again later."
    
    def health_check(self) -> Dict[str, Any]:
        """Health check"""
        try:
            # Test API connection
            test_messages = [{"role": "user", "content": "Hello"}]
            self._call_nvidia_api(test_messages)
            
            return {
                "status": "healthy",
                "model": self.model,
                "api_available": True
            }
        except Exception as e:
            # Even if API unavailable, simplified response is still available
            return {
                "status": "healthy",  # Changed to healthy because simplified response is available
                "model": self.model,
                "api_available": False,
                "fallback_mode": True,
                "error": str(e)
            }

# Global instance
llm_service = LLMService()